=== Transducers

==== Data transformation

ClojureScript offers a rich vocabulary for data transformation in terms of the sequence abstraction,
which makes such transformations highly general and composable. Let's see how we can combine several
collection processing functions to build new ones, we will be using a simple problem throughout this
section: splitting grape clusters, filtering out the rotten ones and cleaning them. We have a collection
of grape clusters like the following:

[source, clojure]
----
(def grape-clusters [{:grapes [{:rotten? false :clean? false}
                               {:rotten? true :clean? false}]
                      :color :green}
                     {:grapes [{:rotten? true :clean? false}
                               {:rotten? false :clean? false}]
                      :color :black}])
----

We are interested in splitting the grape clusters into individual grapes, discarding the rotten ones
and cleaning the remaining grapes so they are ready for eating them. We are well-equiped in ClojureScript
for this data transformation task, we could implement it using the familiar `map`, `filter` and `mapcat`
functions:

[source, clojure]
----
(defn split-cluster
  [c]
  (:grapes c))

(defn not-rotten
  [g]
  (not (:rotten? g)))

(defn clean-grape
  [g]
  (assoc g :clean? true))

(->> grape-clusters
     (mapcat split-cluster)
     (filter not-rotten)
     (map clean-grape))
;; => ({rotten? false :clean? true} {:rotten? false :clean? true})
----

In the above example we succintly solved the problem of selecting and cleaning the grapes and
we can even abstract such transformation combining the `mapcat`, `filter` and `map` operations
using partial application and function composition:

[source, clojure]
----
(def process-clusters
  (comp
    (partial map clean-grape)
    (partial filter not-rotten)
    (partial mapcat split-cluster)))

(process-clusters grape-clusters)
;; => ({rotten? false :clean? true} {:rotten? false :clean? true})
----

The code is very clean but it has a few problems. For example, each call to `map`, `filter` and
`mapcat` consumes and produces a sequence that, although lazy, generate intermediate results
that will be discarded. Each sequence is fed to the next step, which also returns a
sequence. Wouldn't be great if we did the transformation in a single transversal of the `grape-cluster`
collection?

Another problem is that even though our `process-clusters` function works with any sequence we
can't reuse it with anything that is not a sequence. Imagine that instead of having the grape cluster
collection available in memory it is being pushed to us asynchronously in a stream. In that situation
we couldn't reuse `process-clusters` since usually `map`, `filter` and `mapcat` have concrete
implementations depending on the type.

==== Generalizing to process transformations

The process of mapping, filtering or mapcatting isn't necesarily tied to a concrete type but we
keep reimplementing them for different types. Let's see how we can generalize such processes to
be context independent. We'll start implementing naive versions of `map` and `filter` first to
see how they work internally:

[source, clojure]
----
(defn my-map
  [f coll]
  (when-let [s (seq coll)]
    (cons (f (first s)) (map f (rest s)))))

(my-map inc [0 1 2])
;; => (1 2 3)

(defn my-filter
  [pred coll]
  (when-let [s (seq coll)]
    (let [f (first s)
          r (rest s)]
      (if (pred f)
        (cons f (filter pred r))
        (filter pred r)))))

(my-filter odd? [0 1 2])
;; => (1)
----

As we can see, they both assume that they receive a seqable and return a sequence. Like many recursive
functions they can be implemented in terms of the already familiar `reduce` function. Note that functions
that are given to reduce receive an accumulator and an input and return the next accumulator. We'll call
these types of functions reducing functions from now on.

[source, clojure]
----
(defn my-mapr
  [f coll]
  (reduce (fn [acc input]         ;; reducing function
            (conj acc (f input)))
          []                      ;; initial value
          coll))                  ;; collection to reduce

(my-mapr inc [0 1 2])
;; => [1 2 3]

(defn my-filterr
  [pred coll]
  (reduce (fn [acc input]         ;; reducing function
            (if (pred input)
              (conj acc input)
              acc))
          []                      ;; initial value
          coll))                  ;; collection to reduce

(my-filterr odd? [0 1 2])
;; => [1]
----

We've made the previous versions more general since using `reduce` makes our functions work on any thing
that is reducible, not just sequences. However you may have noticed that, even though `my-mapr` and `my-filterr`
don't know anything about the source (`coll`) they are still tied to the output they produce (a vector) both
with the initial value of the reduce (`[]`) and the hardcoded `conj` operation in the body of the reducing
function. We could have accumulated results in another data structure, for example a lazy sequence, but
we'd have to rewrite the functions for doing so.

How can we make these functions truly generic? They shouldn't know about neither the source of inputs they
are transforming nor the output that is generated. Have you noticed that `conj` is just another reducing
function? It takes an accumulator and an input and returns another accumulator. So, if we parameterise
the reducing function that `my-mapr` and `my-filterr` use, they won't know anything about the type of the
result they are building. Let's give it a shot:

[source, clojure]
----
(defn my-mapt
  [f]                         ;; function to map over inputs
  (fn [rfn]                   ;; parameterised reducing function
    (fn [acc input]           ;; transformed reducing function, now it maps `f`!
      (rfn acc (f input)))))

(def incer (my-mapr inc))

(reduce (incer conj) [] [0 1 2])
;; => [1 2 3]

(defn my-filtert
  [pred]                      ;; predicate to filter out inputs
  (fn [rfn]                   ;; parameterised reducing function
    (fn [acc input]           ;; transformed reducing function, now it discards values based on `pred`!
      (if (pred input)
        (rfn acc input)
        acc))))

(def only-odds (my-filtert odd?))

(reduce (only-odds conj) [] [0 1 2])
;; => [1]
----

That's a lot of higher-order functions so let's break it down for a better understanding of what's going
on. We'll examine how `my-mapt` works step by step, the mechanics are similar for `my-filtert` so we'll
leave it out for now.

First of all, `my-mapt` takes a mapping function, in the example we are giving it the `inc` and getting
another function back. Let's substitue `f` with `inc` to see what we are building:

[source, clojure]
----
(def incer (my-mapr inc))
;; (fn [rfn]
;;   (fn [acc input]
;;     (rfn acc (inc input))))
;;               ^^^
----

The resulting function is still parameterised to receive a reducing function to which it will delegate,
let's see what happens when we call it with `conj`:

[source, clojure]
----
(incer conj)
;; (fn [acc input]
;;   (conj acc (inc input)))
;;    ^^^^
----

We get back a reducing function which uses `inc` to transform the inputs and the `conj` reducing function
to accumulate the results. In essence, we have defined map as the transformation of a reducing function.
The functions that transforms a reducing function into another are called transducers in ClojureScript.

To ilustrate the generality of transducers, let's use a different sources and destinations in our call
to `reduce`:

[source, clojure]
----
(reduce (incer str) "" [0 1 2])
;; => "123"

(reduce (only-odds str) "" '(0 1 2))
;; => "1"
----

The transducer versions of `map` and `filter` transform a process that carries inputs from a source to a
destination but don't know anything about where the inputs come from and where they end up. In their
implementation they contain the _essence_ of what they accomplish, independent of context.

Now that we know more about transducers we can try to implement our own version of `mapcat`. We already have
a fundamental piece of it: the `map` transducer. What `mapcat` does is map a function over an input and flatten
the resulting structure one level. Let's try to implemt the catenation part as a transducer:

[source, clojure]
----
(defn my-cat
  [rfn]
  (fn [acc input]
    (reduce rfn acc input)))

(reduce (my-cat conj) [] [[0 1 2] [3 4 5]])
;; => [0 1 2 3 4 5]
----

The `cat` transducer returns a reducing function that catenates its inputs into the accumulator. It does so
reducing the `input` reducible with the `rfn` reducing function and using the accumulator (`acc`) as the
initial value for such reduction. `mapcat` is simply the composition of `map` and `cat`. The order in which
transducers are composed may seem backwards but it'll become clear in a moment.

[source, clojure]
----
(defn my-mapcat
  [f]
  (comp (my-mapt f) my-cat))

(defn dupe
  [x]
  [x x])

(def duper (my-mapcat dupe))

(reduce (duper conj) [] [0 1 2])
;; => [0 0 1 1 2 2]
----

==== Transducers in ClojureScript core

Some of the ClojureScript core functions like `map`, `filter` and `mapcat` support an arity 1 versions
that returns a transducer. Let's revisit our definition of `process-cluster` and define it in terms of
transducers:

[source, clojure]
----
(def process-clusters
  (comp
    (mapcat split-cluster)
    (filter not-rotten)
    (map clean-grape)))
----

A few things changed since our previous definition `process-clusters`. First of all, we are using the
transducer-returning versions of `mapcat`, `filter` and `map` instead of partially applying them for
working on sequences.

Also you may have noticed that the order in which they are composed is reversed, they appear in the order
they are executed. Note that all `map`, `filter` and `mapcat` return a transducer. `filter` transforms the
transducer returned by `map`, applying the filtering before proceeding; `mapcat` transforms the transducer
returned by `filter`, applying the mapping and catenation before proceeding.

One of the powerful properties of transducers is that they are combined using regular function composition.
What's even more elegant is that the composition of various transducers is itself a transducer! This means
that our `process-cluster` is a transducer too, so we have defined a composable and context-independent
algorithmic transformation.

Many of the core ClojureScript functions accept a transducer, let's look at some examples with our newly
created `process-cluster`:

[source, clojure]
----
(into [] process-clusters grape-clusters)
;; => [{:rotten? false, :clean? true} {:rotten? false, :clean? true}]

(sequence process-clusters grape-clusters)
;; => ({:rotten? false, :clean? true} {:rotten? false, :clean? true})

(reduce (process-clusters conj) [] grape-clusters)
;; => [{:rotten? false, :clean? true} {:rotten? false, :clean? true}]
----

Since using `reduce` with the reducing function returned from a transducer is so common, there is
a function for reducing with a transformation called `transduce`. We can now rewrite the previous call
to `reduce` using `transduce`:

[source, clojure]
----
(transduce process-clusters conj [] grape-clusters)
;; => [{:rotten? false, :clean? true} {:rotten? false, :clean? true}]
----

==== Initialisation

In the last example we provided an initial value to the `transduce` function (`[]`) but we can omit it
and get the same result:

[source, clojure]
----
(transduce process-clusters conj grape-clusters)
;; => [{:rotten? false, :clean? true} {:rotten? false, :clean? true}]
----

What's going on here? How can `transduce` know what initial value use as an accumulator when we haven't
specified it? Try calling `conj` without any arguments and see what happens:

[source, clojure]
----
(conj)
;; => []
----

The `conj` function has a arity 0 version that returns an empty vector but is not the only reducing function
that supports arity 0. Let's explore some others:

[source, clojure]
----
(+)
;; => 0

(*)
;; => 1

(str)
;; => ""

(= identity (comp))
;; => true
----

The reducing function returned by transducers must support the arity 0 as well, which will tipically delegate
to the transformed reducing function. There is no sensible implementation of the arity 0 for the transducers
we have implemented so far so we'll simply call the reducing function without arguments. Here's how our
modified `my-mapt` could look like:

[source, clojure]
----
(defn my-mapt
  [f]
  (fn [rfn]
    (fn
      ([] (rfn))                ;; arity 0 that delegates to the reducing fn
      ([acc input]
        (rfn acc (f input))))))
----

The call to the arity 0 of the reducing function returned by a transducer will call the arity 0 version of
every nested reducing function, eventually calling the outermost reducing function. Let's see an example with
our already defined `process-clusters` transducer:

[source, clojure]
----
((process-clusters conj))
;; => []
----

The call to the arity 0 flows through the transducer stack, eventually calling `conj`.

==== Stateful transducers

So far we've only seen purely functional transducers, they don't have any implicit state and are very
predictable. However, there are many data transformation functions that are inherently stateful like
`take`. `take` receives a number `n` of elements to keep and a collection and returns a collection with at
most `n` elements.

[source, clojure]
----
(take 10 (range 100))
;; => (0 1 2 3 4 5 6 7 8 9)
----

Let's step back for a bit and learn about the early termination of the `reduce` function. We can wrap an
accumulator in a type called `Reduced` for telling `reduce` that the reduction process should terminate
immediately. Let's see an example of a reduction that aggregates the inputs in a collection and finishes
as soon as there are 10 elements in the accumulator:

[source, clojure]
----
(reduce (fn [acc input]
          (if (= (count acc) 10)
            (reduced acc)
            (conj acc input)))
         []
         (range 100))
;; => [0 1 2 3 4 5 6 7 8 9]
----

Since transducers are modifications of reducing functions they also use `reduced` for early termination.
Note that stateful transducers may need to do some cleanup before the process terminates, so they
must support an arity 1 as a "completion" step. Usually, like with arity 0, this arity will simply delegate
to the transformed reducing function's arity 1.

Knowing this we are able to write stateful transducers like `take`, we'll be using mutable state internally
for tracking the number of inputs we have seen so far, and wrap the accumulator in a `reduced` as soon as
we've seen enough elements:

[source, clojure]
----
(defn my-take
  [n]
  (fn [rfn]
    (let [remaining (volatile! n)]
      (fn
        ([] (rfn))
        ([acc] (rfn acc))
        ([acc input]
          (let [rem @remaining
                nr (vswap! remaining dec)
                result (if (pos? rem)
                         (rfn acc input)   ;; we still have items to take
                         acc)]             ;; we're done, acc becomes the result
            (if (not (pos? nr))
              (ensure-reduced result)      ;; wrap result in reduced if not already
              result)))))))
----

This is a simplified version of the `take` function present in ClojureScript core. There are
a few things to note here so let's break it up in pieces to understand it better.

The first thing to notice is that we are creating a mutable value inside the transducer. Note
that we don't create it until we receive a reducing function to transform. If we created it before
returning the transducer we couldn't use `my-take` more than once. Since the transducer is handed
a reducing function to transform each time it is used, we can use it multiple times and the mutable
variable will be created in every use.

[source, clojure]
----
(fn [rfn]
  (let [remaining (volatile! n)] ;; make sure to create mutable variables inside the transducer
    (fn
      ;; ...
)))

(def take-five (my-take 5))

(transduce take-five conj (range 100))
;; => [0 1 2 3 4]

(transduce take-five conj (range 100))
;; => [0 1 2 3 4]
----

Let's now dig into the reducing function returned from `my-take`. First of all we `deref` the volatile
to get the number of elements that remain to be taken and decrement it to get the next remaining value.
If there are still remaining items to take, we call `rfn` passing the accumulator and input; if not, we
already have the final result.

[source, clojure]
----
([acc input]
  (let [rem @remaining
        nr (vswap! remaining dec)
        result (if (pos? rem)
                 (rfn acc input)
                 acc)]
    ;; ...
))
----

The body of `my-take` should be obvious by now. We check whether there are still items to be processed
using the next remainder (`nr`) and, if not, wrap the result in a `reduced` using the `ensure-reduced`
function. `ensure-reduced` will wrap the value in a `reduced` if it's not reduced already or simply return
the value if it's already reduced. In case we are not done yet, we return the accumulated `result` for
further processing.

[source, clojure]
----
(if (not (pos? nr))
  (ensure-reduced result)
  result)
----

We've seen an example of a stateful transducer but it didn't do anything in its completion step. Let's
see an example of a transducer that uses the completion step to flush an accumulated value. We'll
implement a simplified version of `partition-all`, which given a `n` number of elements converts the inputs
in vectors of size `n`. For understanding its purpose better let's see what the arity 2 version gives us when
providing a number and a collection:

[source, clojure]
----
(partition-all 3 (range 10))
;; => ((0 1 2) (3 4 5) (6 7 8) (9))
----

The transducer returning function of `partition-all` will take a number `n` and return a transducer that groups
inputs in vectors of size `n`. In the completion step it will check if there is an accumulated result and, if so,
add it to the result. Here's a simplified version of ClojureScript core `partition-all` function, where `array-list`
is a wrapper for a mutable JavaScript array:

[source, clojure]
----
(defn my-partition-all
  [n]
  (fn [rfn]
    (let [a (array-list)]
      (fn
        ([] (rfn))
        ([result]
          (let [result (if (.isEmpty a)                  ;; no inputs accumulated, don't have to modify result
                         result
                         (let [v (vec (.toArray a))]
                           (.clear a)                    ;; flush array contents for garbage collection
                           (unreduced (rfn result v))))] ;; pass to `rfn`, removing the reduced wrapper if present
            (rfn result)))
        ([acc input]
          (.add a input)
          (if (== n (.size a))                           ;; got enough results for a chunk
            (let [v (vec (.toArray a))]
              (.clear a)
              (rfn acc v))                               ;; the accumulated chunk becomes input to `rfn`
            acc))))))

(def triples (my-partition-all 3))

(transduce triples conj (range 10))
;; => [[0 1 2] [3 4 5] [6 7 8] [9]]
----

==== Functions for working with transducers

- preserve-reduced
- completing
- transduce
- eduction
- run!

==== Transducible processes

- transducible processes

=== Transients

Although ClojureScript's immutable and persistent data structures are reasonably performant
there are situations in which we are transforming large data structures using multiple steps
to only share the final result. For example, the core `into` function takes a collection and eagerly
populates it with the contents of a sequence:

[source, clojure]
----
(into [] (range 100))
;; => [0 1 2 ... 98 99]
----

In the above example we are generating a vector of 100 elements `conj`-ing one at a time. Every
intermediate vector that is not the final result won't be seen by anybody except the `into`
function and the array copying required for persistence is an unnecesary overhead.

For these situations ClojureScript provides a special version of some of its persistent data
structures, which are called transients. Maps, vectors and sets have a transient counterpart.
Transients are always derived from a persistent data structure using the `transient` function,
which creates a transient version in constant time:

[source, clojure]
----
(def tv (transient [1 2 3]))
;; => #<[object Object]>
----

Transients support the read API of their persistent counterparts:

[source, clojure]
----
(def tv (transient [1 2 3]))

(nth tv 0)
;; => 1

(get tv 2)
;; => 3

(def tm (transient {:language "ClojureScript"}))

(:language tm)
;; => "ClojureScript"

(def ts (transient #{:a :b :c}))

(contains? ts :a)
;; => true

(:a ts)
;; => :a
----

Since transients don't have persistent and immutable semantics for updates they can't be transformed
using the already familiar `conj` or `assoc` functions. Instead, the transforming functions that work
on transients end with a bang. Let's look at an example using `conj!` on a transient:

[source, clojure]
----
(def tv (transient [1 2 3]))

(conj! tv 4)
;; => #<[object Object]>

(nth tv 3)
;; => 4
----

As you can see, the transient version of the vector is neither immutable or persistent. Instead, the
vector is mutated in place. Although we could transform `tv` repeatedly using `conj!` on it we shouldn't
abandon the idioms used with the persistent data structures: when transforming a transient, use the
returned version of it for further modifications like in the following example:

[source, clojure]
----
(-> [1 2 3]
  transient
  (conj! 4)
  (conj! 5))
;; => #<[object Object]>
----

We can convert a transient back to a persistent and immutable data structure by calling `persistent!` on
it. This operation, like deriving a transient from a persistent data structure, is done in constant time.

[source, clojure]
----
(-> [1 2 3]
  transient
  (conj! 4)
  (conj! 5)
  persistent!)
;; => [1 2 3 4 5]
----

A peculiarity of transforming transients into persistent structures is that the transient version is
invalidated after being converted to a persistent data structure and we can't do further transformations
to it. This happens because the derived persistent data structure uses the transient's internal nodes
and mutating them would break the immutability and persistent guarantees:

[source, clojure]
----
(def tm (transient {}))
;; => #<[object Object]>

(assoc! tm :foo :bar)
;; => #<[object Object]>

(persistent! tm)
;; => {:foo :bar}

(assoc! tm :baz :frob)
;; Error: assoc! after persistent!
----

Going back to our initial example with `into`, here's a very simplified implementation of it that uses
a transient for performance, returning a persistent data structure and thus exposing a purely functional
interface although it uses mutation internally:

[source, clojure]
----
(defn my-into
  [to from]
  (persistent! (reduce conj! (transient to) from)))

(my-into [] (range 100))
;; => [0 1 2 ... 98 99]
----


=== Metadata

TBD


=== Macros

////
Intends to be a little explanation to macros (an extensive documentation is not a goal,
because it fits perfectly into its own book) and the peculiarities of the clojurescript
in respect to the clojure.
////

TBD


=== Core protocols

////
As clojurescript in difference with Clojure defines everything in terms of protocols and this
subchapter intends to expain many of these protocols and how them can be used by the user.
////

TBD


=== CSP & core.async

////
Intends to be an comprensive introduction to csp and core.async. We are aware that core.async
is not part of core of clojurescript but it is widely used and interesting concepts like that
would be awesome to cover in the book.
////

TBD
